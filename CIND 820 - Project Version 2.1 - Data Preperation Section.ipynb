{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5bdb1d",
   "metadata": {},
   "source": [
    "# CIND 820 - Deliverable 3 - Data Preparation\n",
    "## Data Preparation Section\n",
    "\n",
    "The purpose of this section of the notebook is to setup our data such that it becomes ready for data analysis and ML model adoption. Since this project is largely text-based, we must engineer some features first before we can begin our analysis. As such, breaking convention, our project will start with the *Data Preparation Section* first before moving into the *Data Analysis Section*. \n",
    "\n",
    "1. First we need to fix the 'stars' column. It's formatted as Float64 and we want integers for a cleaner grouping later. \n",
    "\n",
    "2. Second, we need to prepare our 'text' column such that it is formatted in a way that is acceptable for both our VADER-library and Countvectorizer library for the purposes of feature engineering. We need to create 2 versions of the 'text' column': One for the VADER library to read and generate sentiment scores, and another for the Countvectorizer library to create additional Bag-of-Words vectors for our model training. We require two different columns because the level of data pre-processing required for each are different. \n",
    "\n",
    "3. Third, we'll need to employ stratified sampling to balance out our star-ratings. We will also further reduce the size of our dataset for the purposes of model training.\n",
    "\n",
    "4. Fourth, we'll engineer VADER-based sentiment scores using the VADER library\n",
    "\n",
    "5. Then, we'll create Bag-of-Words vectors, using the Countvectorizer library, which will be used as additional features for our model. \n",
    "\n",
    "The completion of these 5 steps will mark the end of the *Data Preparation Section* as the data will be ready for analysis and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6f468",
   "metadata": {},
   "source": [
    "### Setting up the Notebook Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965fcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Sunora/iCloudDrive/Documents/Learning Data Analytics/TMU Certificate copy/CIND 820/Yelp Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963e60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 349514 entries, 0 to 349513\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   review_id    349514 non-null  object \n",
      " 1   user_id      349514 non-null  object \n",
      " 2   business_id  349514 non-null  object \n",
      " 3   stars        349514 non-null  float64\n",
      " 4   text         349514 non-null  object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 13.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries and setting up environment\n",
    "import pandas as pd # Used for Data Manipulation\n",
    "import numpy as np # Used for Numerical Operations\n",
    "import re # Used for Regular Expressions\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # Used for Sentiment Analysis (VADER)\n",
    "# nltk.download('vader_lexicon') # Downloading VADER Lexicon\n",
    "import spacy # Used for Advanced NLP tasks (tokenization, lemmatization, etc.)\n",
    "nlp = spacy.load('en_core_web_sm') # Loading SpaCy English Model\n",
    "\n",
    "#Defining working directory and reading data\n",
    "filepath = r'C:\\Users\\Sunora\\iCloudDrive\\Documents\\Learning Data Analytics\\TMU Certificate copy\\CIND 820\\Yelp Dataset\\yelp_review_sample_data.csv'\n",
    "originalData = pd.read_csv(filepath)\n",
    "\n",
    "# Previewing the Datatype Info\n",
    "originalData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f443b5f",
   "metadata": {},
   "source": [
    "### 1. Fixing Stars Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6899e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Converting 'stars' column from Float64 to Integer\n",
    "originalData['stars'] = originalData['stars'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eba7ca",
   "metadata": {},
   "source": [
    "### 2. Text Pre-processing for VADER and Bag-of-Words Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38812cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Cleaning 'text' data column and and performing text preprocessing\n",
    "\n",
    "# General cleaning of 'text' column\n",
    "originalData['text'] = originalData['text'].fillna('').astype(str) # Fill NaN values with empty strings and ensure all entries are strings\n",
    "\n",
    "# Creating a function to perform basic text cleaning - applicable to both VADER and Bag-of-Words (BoW)\n",
    "def text_clean_general(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags (if any)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space (meaning double-spaces, new lines, tabs, etc.)\n",
    "    return text.strip() # Remove leading/trailing spaces\n",
    "\n",
    "# Applying the function defined above to create a new text column that is pre-processed for VADER analysis\n",
    "originalData['text_VADER'] = originalData['text'].apply(text_clean_general) # Creating a new column 'text_VADER' for VADER analysis\n",
    "\n",
    "# Loop for cleaning the 'text' column for BoW - to be used on the 'text_VADER' column as BoW needs more aggressive cleaning\n",
    "corpus = originalData['text_VADER'].str.lower().tolist() # Converting the 'text_VADER' column to a list for processing; using VADER cleaned text as base\n",
    "outputs = [] # List to store cleaned text and token counts - for easier dataframe conversion \n",
    "for doc in nlp.pipe(corpus, batch_size=1000, n_process=8): # Using nlp.pipe for efficient processing (due to size of dataset)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha] # Lemmatization, removing stop words and non-alphabetic tokens\n",
    "    cleaned_text = ' '.join(tokens) # Joining tokens back into a single string\n",
    "    token_count = len(tokens) # Counting number of tokens after cleaning - an additional feature\n",
    "    outputs.append((cleaned_text, token_count)) # Appending cleaned text and token count as a tuple to outputs list\n",
    "\n",
    "# Gathering outputs from the loop (above) into the original dataframe as new columns\n",
    "originalData[['text_BoW', 'num_Tokens']] = pd.DataFrame(outputs, index = originalData.index) # text_BoW for Bag of Words cleaned text, num_Tokens for number of tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27982771",
   "metadata": {},
   "source": [
    "### 3. Stratified Sampling for Star Rating Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f82814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunora\\AppData\\Local\\Temp\\ipykernel_20768\\1940115590.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n = 5000, random_state = 2025))\n"
     ]
    }
   ],
   "source": [
    "# Using balancedData to store the new balanced originalDataset based on stratified sampling - 5000 samples per star rating\n",
    "balancedData = (\n",
    "    originalData[originalData['num_Tokens'] > 0]  # Filtering out rows with zero tokens after cleaning\n",
    "    .groupby('stars', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n = 5000, random_state = 2025))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54151d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review_id    25000 non-null  object\n",
      " 1   user_id      25000 non-null  object\n",
      " 2   business_id  25000 non-null  object\n",
      " 3   stars        25000 non-null  int64 \n",
      " 4   text         25000 non-null  object\n",
      " 5   text_VADER   25000 non-null  object\n",
      " 6   text_TFIDF   25000 non-null  object\n",
      " 7   num_Tokens   25000 non-null  int64 \n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "balancedData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94386db",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering VADER-Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c614966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>text_VADER</th>\n",
       "      <th>text_TFIDF</th>\n",
       "      <th>num_Tokens</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoCim4CRm-WCoU-CFfWpLw</td>\n",
       "      <td>McdCFYocB1hFIiDQBRQ7YA</td>\n",
       "      <td>P_nqb7lULOtx3pAJbKfFXA</td>\n",
       "      <td>1</td>\n",
       "      <td>Santa Fe used to be my favorite restaurant. I ...</td>\n",
       "      <td>Santa Fe used to be my favorite restaurant. I ...</td>\n",
       "      <td>santa fe favorite restaurant enjoy cancun taco...</td>\n",
       "      <td>48</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.0209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8s6Eejmy24XUhgNkR2uIUA</td>\n",
       "      <td>X67DbQdqHeZ-F2UVUOhn1g</td>\n",
       "      <td>WNjrsnJVPPnv_FtHHdjklA</td>\n",
       "      <td>1</td>\n",
       "      <td>I have never experienced this level of incompe...</td>\n",
       "      <td>I have never experienced this level of incompe...</td>\n",
       "      <td>experience level incompetence customer service...</td>\n",
       "      <td>84</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.6697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2GdPCXF_5fR4_od5DJTD8Q</td>\n",
       "      <td>-VPeYf78MNJAB0iR7d9-zg</td>\n",
       "      <td>QboMIy08NLnBbLXEsmnDHg</td>\n",
       "      <td>1</td>\n",
       "      <td>I've noticed a \"Rising sun\" flag displayed in ...</td>\n",
       "      <td>I've noticed a \"Rising sun\" flag displayed in ...</td>\n",
       "      <td>notice rise sun flag display restaurant symbol...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.9612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vYSCzz-jM7ibdoIUCRLysw</td>\n",
       "      <td>I0Vt1g8iK0D_cxXkJyXb0A</td>\n",
       "      <td>INz7vujcHs0AggsV__pXYQ</td>\n",
       "      <td>1</td>\n",
       "      <td>Sold me a part that was wrong size and wouldn'...</td>\n",
       "      <td>Sold me a part that was wrong size and wouldn'...</td>\n",
       "      <td>sell wrong size exchange get rob</td>\n",
       "      <td>6</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mLokfOcquwIP57pcOkHBZQ</td>\n",
       "      <td>TV3p-bv5yh8RgdJ3WxM7Ug</td>\n",
       "      <td>eh8WfQqPa2ZWtbXe9_wHgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>I brought my car to Hyundai Service for a chec...</td>\n",
       "      <td>I brought my car to Hyundai Service for a chec...</td>\n",
       "      <td>bring car hyundai service check engine light c...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.3201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  CoCim4CRm-WCoU-CFfWpLw  McdCFYocB1hFIiDQBRQ7YA  P_nqb7lULOtx3pAJbKfFXA   \n",
       "1  8s6Eejmy24XUhgNkR2uIUA  X67DbQdqHeZ-F2UVUOhn1g  WNjrsnJVPPnv_FtHHdjklA   \n",
       "2  2GdPCXF_5fR4_od5DJTD8Q  -VPeYf78MNJAB0iR7d9-zg  QboMIy08NLnBbLXEsmnDHg   \n",
       "3  vYSCzz-jM7ibdoIUCRLysw  I0Vt1g8iK0D_cxXkJyXb0A  INz7vujcHs0AggsV__pXYQ   \n",
       "4  mLokfOcquwIP57pcOkHBZQ  TV3p-bv5yh8RgdJ3WxM7Ug  eh8WfQqPa2ZWtbXe9_wHgQ   \n",
       "\n",
       "   stars                                               text  \\\n",
       "0      1  Santa Fe used to be my favorite restaurant. I ...   \n",
       "1      1  I have never experienced this level of incompe...   \n",
       "2      1  I've noticed a \"Rising sun\" flag displayed in ...   \n",
       "3      1  Sold me a part that was wrong size and wouldn'...   \n",
       "4      1  I brought my car to Hyundai Service for a chec...   \n",
       "\n",
       "                                          text_VADER  \\\n",
       "0  Santa Fe used to be my favorite restaurant. I ...   \n",
       "1  I have never experienced this level of incompe...   \n",
       "2  I've noticed a \"Rising sun\" flag displayed in ...   \n",
       "3  Sold me a part that was wrong size and wouldn'...   \n",
       "4  I brought my car to Hyundai Service for a chec...   \n",
       "\n",
       "                                          text_TFIDF  num_Tokens    neg  \\\n",
       "0  santa fe favorite restaurant enjoy cancun taco...          48  0.119   \n",
       "1  experience level incompetence customer service...          84  0.110   \n",
       "2  notice rise sun flag display restaurant symbol...          27  0.276   \n",
       "3                   sell wrong size exchange get rob           6  0.222   \n",
       "4  bring car hyundai service check engine light c...          44  0.055   \n",
       "\n",
       "     neu    pos  compound  \n",
       "0  0.779  0.102   -0.0209  \n",
       "1  0.819  0.071   -0.6697  \n",
       "2  0.690  0.034   -0.9612  \n",
       "3  0.778  0.000   -0.6449  \n",
       "4  0.919  0.026   -0.3201  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate VADER sentiment scores\n",
    "sia = SentimentIntensityAnalyzer() # Initializing VADER Sentiment Intensity Analyzer\n",
    "balancedData['vader_scores'] = balancedData['text_VADER'].apply(sia.polarity_scores) # Applying VADER to the 'text_VADER' column and storing results in a new column 'vader_scores'\n",
    "balancedData = pd.concat([balancedData.drop(['vader_scores'], axis=1), balancedData['vader_scores'].apply(pd.Series)], axis=1) # Expanding the 'vader_scores' dictionary into separate columns\n",
    "\n",
    "balancedData.head() # Previewing the final balanced dataset with VADER scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b536d9e",
   "metadata": {},
   "source": [
    "### 5. Creating Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words Feature Engineering\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "balancedDataBoW = balancedData.copy() # Creating a copy of balancedData for Bag-of-Words processing\n",
    "\n",
    "# Creating Bag-of-Words Matrix\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_features = 5000, # Limiting to top 5000 features to manage dimensionality\n",
    "    ngram_range= (1, 1), # Using unigrams only,\n",
    "    # min_df = 0.03, # Minimum document frequency of 5%\n",
    "    # max_df = 0.75 # Maximum document frequency of 75%\n",
    "    )\n",
    "\n",
    "# Creating Bag-of-Words matrix\n",
    "bow_matrix = bow_vectorizer.fit_transform(balancedDataBoW['text_TFIDF']) # Fitting and transforming the Bag-of-Words matrix\n",
    "\n",
    "# Adding Bag-of-Words features to balancedDataBoW DataFrame\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    index=balancedDataBoW.index,\n",
    "    columns=bow_vectorizer.get_feature_names_out()\n",
    ")\n",
    "balancedDataBoW = pd.concat([balancedDataBoW.reset_index(drop=True), bow_df.reset_index(drop=True)], axis=1) # Concatenating the Bag-of-Words features to the balancedDataBoW DataFrame\n",
    "balancedDataBoW.drop(columns=['text', 'text_VADER', 'text_BoW'], inplace=True) # Dropping text columns to reduce file size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c72932e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5008)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of the Bag-of-Words DataFrame\n",
    "balancedDataBoW.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c02e8",
   "metadata": {},
   "source": [
    "### Saving Cleaned Dataset as separate .CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73c8194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the balancedDataBoW data to a new CSV file to prevent future re-runs of the data cleaning step.\n",
    "balancedDataBoW.to_csv('balancedDataBoW.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebde3a",
   "metadata": {},
   "source": [
    "## End of Data Preparation Section\n",
    "This marks the end of the *Data Preparation Section*. In summary, we've cleaned our original dataset, balanced our star-ratings through stratified sampling, feature engineered sentiment scores using VADER and using a Bag-of-Words matrix. We then dropped the 'text', 'text_VADER', and 'text_TFIDF' columns which are no longer required as we've already created vectorized features from the text. The final cleaned dataset is then saved as balancedDataBoW (for Bag-of-Words) / balancedDataTFIDF (for TF-IDF) csv files. \n",
    "\n",
    "### Next Steps\n",
    "In the next phase, the *Data Analysis Section* we will further examine and explore this balancedData csv file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
